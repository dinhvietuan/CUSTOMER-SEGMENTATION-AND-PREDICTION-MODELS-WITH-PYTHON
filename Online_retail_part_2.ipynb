{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee77227",
   "metadata": {},
   "source": [
    "# Customer Purchase Prediction using Machine Learning Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa9c8d",
   "metadata": {},
   "source": [
    "## IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52186b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import data \n",
    "import pandas as pd\n",
    "\n",
    "# Read excel file\n",
    "df_retail = pd.read_csv('C:/Users/Darius/Desktop/PROJECT//online_retail_II.csv')\n",
    "\n",
    "\n",
    "df_retail = df_retail.rename(columns={'Customer ID': 'CustomerID'})\n",
    "\n",
    "# Convert 'InvoiceDate' column to datetime format \n",
    "# and display information about the 'df_retail' DataFrame\n",
    "df_retail['InvoiceDate'] = pd.to_datetime(df_retail['InvoiceDate'])\n",
    "\n",
    "# extract only the date part and convert it to date data type\n",
    "df_retail['InvoiceDate'] = df_retail['InvoiceDate'].dt.date\n",
    "\n",
    "df_retail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5897c",
   "metadata": {},
   "source": [
    "## Filter Data from UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data from UK\n",
    "tx_uk = df_retail.query(\"Country == 'United Kingdom'\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e677c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tx_uk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8afe72",
   "metadata": {},
   "source": [
    "## CHOOSE DATA FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the Python date objects to Pandas datetime objects\n",
    "start_date = pd.to_datetime('2010-09-01')\n",
    "end_date = pd.to_datetime('2011-09-01')\n",
    "\n",
    "# Filter 6 months from 03/2011 to 09/2011\n",
    "tx_12m = tx_uk[(tx_uk.InvoiceDate < end_date) & (tx_uk.InvoiceDate >= start_date)].reset_index(drop = True)\n",
    "tx_12m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5996a",
   "metadata": {},
   "source": [
    "### CHOOSE DATA FOR TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b465d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Python date objects to Pandas datetime objects\n",
    "start_date = pd.to_datetime('2011-09-01')\n",
    "end_date = pd.to_datetime('2011-12-01')\n",
    "\n",
    "# Filter 6 months from 06/2011 to 12/2011\n",
    "tx_next = tx_uk[(tx_uk.InvoiceDate < end_date) & (tx_uk.InvoiceDate >= start_date)].reset_index(drop = True)\n",
    "tx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf01839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the tx_next dataframe\n",
    "tx_next['InvoiceDate'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame called tx_user that contains a list of unique customer IDs from the tx_12m DataFrame\n",
    "tx_user = pd.DataFrame(tx_12m['CustomerID'].unique())\n",
    "\n",
    "# Rename the column in tx_user to 'CustomerID'\n",
    "tx_user.columns =['CustomerID']\n",
    "\n",
    "# Create a new DataFrame called tx_next_first_purchase that contains the earliest purchase date for each customer in the tx_next DataFrame\n",
    "tx_next_first_purchase = tx_next.groupby('CustomerID').InvoiceDate.min().reset_index()\n",
    "\n",
    "# Rename the columns in tx_next_first_purchase to 'CustomerID' and 'MinPurchaseDate'\n",
    "tx_next_first_purchase.columns = ['CustomerID', 'MinPurchaseDate']\n",
    "\n",
    "# Print the first few rows of the tx_next_first_purchase DataFrame\n",
    "tx_next_first_purchase.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fa0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame called tx_last_purchase that contains the latest purchase date for each customer in the tx_12m DataFrame\n",
    "tx_last_purchase = tx_12m.groupby('CustomerID').InvoiceDate.max().reset_index()\n",
    "\n",
    "# Rename the columns in tx_last_purchase to 'CustomerID' and 'MaxPurchaseDate'\n",
    "tx_last_purchase.columns = ['CustomerID', 'MaxPurchaseDate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the tx_last_purchase and tx_next_first_purchase DataFrames on the 'CustomerID' column using a left join\n",
    "tx_purchase_dates = pd.merge(tx_last_purchase, tx_next_first_purchase, on ='CustomerID', how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf11e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_purchase_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days between the latest purchase date and the earliest purchase date for each customer\n",
    "tx_purchase_dates['NextPurchaseDay'] = (tx_purchase_dates['MinPurchaseDate'] - tx_purchase_dates['MaxPurchaseDate']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da497ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_purchase_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49631690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the tx_user DataFrame with the tx_purchase_dates DataFrame on the 'CustomerID' column using a left join\n",
    "tx_user = pd.merge(tx_user, tx_purchase_dates[['CustomerID','NextPurchaseDay']],  on = 'CustomerID', how ='left')\n",
    "\n",
    "# Show the first 5 rows of the resulting DataFrame\n",
    "tx_user.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values (NaNs) in the 'NextPurchaseDay' column with 999\n",
    "tx_user =tx_user.fillna(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days between each customer's last purchase and the end of the 12-month transaction period, and add the result as a new 'Recency' column to the tx_last_purchase DataFrame\n",
    "tx_last_purchase['Recency'] = (tx_last_purchase['MaxPurchaseDate'].max() - tx_last_purchase['MaxPurchaseDate']).dt.days\n",
    "\n",
    "# Merge the tx_user DataFrame with the tx_last_purchase DataFrame on the 'CustomerID' column\n",
    "tx_user = pd.merge(tx_user, tx_last_purchase[['CustomerID', 'Recency']], on = 'CustomerID')\n",
    "\n",
    "# Show the first 5 rows of the resulting DataFrame\n",
    "tx_user.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe data \n",
    "tx_user.Recency.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee810ed0",
   "metadata": {},
   "source": [
    "### Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530758dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyoff\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize an empty list of SSE values\n",
    "sse = [0] * 10\n",
    "\n",
    "# Extract the Recency column from the df_uk_user_q3 DataFrame\n",
    "tx_recency = tx_user[['Recency']]\n",
    "\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)\n",
    "    tx_recency[\"clusters\"] = kmeans.labels_\n",
    "    sse[k] = kmeans.inertia_\n",
    "# Create a scatter plot of SSE values for each k\n",
    "plot_data = [ \n",
    "    go.Scatter(\n",
    "        x = list(range(1, 10)),\n",
    "        y = sse[1:10]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Set the plot layout\n",
    "plot_layout = go.Layout(\n",
    "    title = 'Number of Clusters of Recency in Q2 and Q3',\n",
    "    xaxis = {'title': 'Number of clusters'},\n",
    "    yaxis = {'title': 'SSE'}\n",
    ")\n",
    "\n",
    "# Create the plot figure\n",
    "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
    "pyoff.plot(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26899344",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(tx_user[['Recency']])\n",
    "tx_user['RecencyCluster'] = kmeans.predict(tx_user[['Recency']])\n",
    "\n",
    "def order_cluster(cluster_field_name, target_field_name,df,ascending):\n",
    "    new_cluster_field_name = 'new_' + cluster_field_name\n",
    "    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n",
    "    df_new = df_new.sort_values(by = target_field_name, ascending = ascending).reset_index(drop = True)\n",
    "    df_new['index'] = df_new.index\n",
    "    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n",
    "    df_final = df_final.drop([cluster_field_name], axis = 1)\n",
    "    df_final = df_final.rename(columns = {\"index\": cluster_field_name})\n",
    "    return df_final\n",
    "\n",
    "print(tx_user)\n",
    "tx_user = order_cluster('RecencyCluster', 'Recency', tx_user, False)\n",
    "tx_user.groupby('RecencyCluster')['Recency'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4785d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency\n",
    "tx_frequency = tx_12m.groupby('CustomerID').InvoiceDate.count().reset_index()\n",
    "tx_frequency.columns = ['CustomerID', 'Frequency']\n",
    "tx_frequency.head()\n",
    "\n",
    "tx_user = pd.merge(tx_user, tx_frequency, on = 'CustomerID')\n",
    "tx_user.head()\n",
    "tx_user.Frequency.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1392312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyoff\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize an empty list of SSE values\n",
    "sse = [0] * 10\n",
    "\n",
    "# Extract the Recency column from the df_uk_user_q3 DataFrame\n",
    "tx_frequency = tx_user[['Frequency']]\n",
    "\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_frequency)\n",
    "    tx_frequency[\"clusters\"] = kmeans.labels_\n",
    "    sse[k] = kmeans.inertia_\n",
    "# Create a scatter plot of SSE values for each k\n",
    "plot_data = [ \n",
    "    go.Scatter(\n",
    "        x = list(range(1, 10)),\n",
    "        y = sse[1:10]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Set the plot layout\n",
    "plot_layout = go.Layout(\n",
    "    title = 'Number of Clusters of Frequency in Q2 and Q3',\n",
    "    xaxis = {'title': 'Number of clusters'},\n",
    "    yaxis = {'title': 'SSE'}\n",
    ")\n",
    "\n",
    "# Create the plot figure\n",
    "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
    "pyoff.plot(fig)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(tx_user[['Frequency']])\n",
    "tx_user['FrequencyCluster'] = kmeans.predict(tx_user[['Frequency']])\n",
    "\n",
    "tx_user = order_cluster('FrequencyCluster', 'Frequency', tx_user, True)\n",
    "tx_user.groupby('FrequencyCluster')['Frequency'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75512f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monetary Value\n",
    "\n",
    "tx_12m['Revenue'] = tx_12m['Price']*tx_12m['Quantity']\n",
    "tx_revenue = tx_12m.groupby('CustomerID').Revenue.sum().reset_index()\n",
    "\n",
    "tx_user = pd.merge(tx_user, tx_revenue, on = 'CustomerID', how='left')\n",
    "tx_user.head()\n",
    "tx_user.Revenue.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyoff\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize an empty list of SSE values\n",
    "sse = [0] * 10\n",
    "\n",
    "# Extract the Recency column from the df_uk_user_q3 DataFrame\n",
    "tx_revenue = tx_user[['Revenue']]\n",
    "\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_revenue)\n",
    "    tx_revenue[\"clusters\"] = kmeans.labels_\n",
    "    sse[k] = kmeans.inertia_\n",
    "# Create a scatter plot of SSE values for each k\n",
    "plot_data = [ \n",
    "    go.Scatter(\n",
    "        x = list(range(1, 10)),\n",
    "        y = sse[1:10]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Set the plot layout\n",
    "plot_layout = go.Layout(\n",
    "    title = 'Number of Clusters of Revenue in Q2 and Q3',\n",
    "    xaxis = {'title': 'Number of clusters'},\n",
    "    yaxis = {'title': 'SSE'}\n",
    ")\n",
    "\n",
    "# Create the plot figure\n",
    "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
    "pyoff.plot(fig)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(tx_user[['Revenue']])\n",
    "tx_user['RevenueCluster'] = kmeans.predict(tx_user[['Revenue']])\n",
    "\n",
    "tx_user = order_cluster('RevenueCluster', 'Revenue', tx_user, True)\n",
    "tx_user.groupby('RevenueCluster')['Revenue'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall Score\n",
    "\n",
    "tx_user['OverallScore'] = tx_user['RecencyCluster'] + tx_user['FrequencyCluster']+tx_user['RevenueCluster']\n",
    "tx_user.groupby('OverallScore')['Recency','Frequency','Revenue'].mean()\n",
    "\n",
    "tx_user.groupby('OverallScore')['Recency'].count()\n",
    "\n",
    "tx_user['Segment'] = 'Low-Value'\n",
    "tx_user.loc[tx_user['OverallScore'] > 2, 'Segment'] = 'Mid-Value'\n",
    "tx_user.loc[tx_user['OverallScore'] > 4, 'Segment'] = 'High-Value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71862551",
   "metadata": {},
   "source": [
    "## ADDING NEW FEATURES TO MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bf20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new Features\n",
    "# Create a dataframe with CustomerID, and Invoice Date\n",
    "\n",
    "tx_day_order = tx_12m[['CustomerID', 'InvoiceDate']]\n",
    "\n",
    "# Convert Invoice Date to day\n",
    "\n",
    "tx_day_order['InvoiceDay'] = pd.to_datetime(tx_12m['InvoiceDate']).dt.date\n",
    "\n",
    "tx_day_order = tx_day_order.sort_values(['CustomerID', 'InvoiceDate'])\n",
    "\n",
    "# Drop Duplicates (If Customer buy more than 2 times in a day)\n",
    "tx_day_order = tx_day_order.drop_duplicates(subset = ['CustomerID', 'InvoiceDay'], keep = 'first')\n",
    "\n",
    "# Shifting last 3 purchase dates\n",
    "tx_day_order['PrevInvoiceDate'] = tx_day_order.groupby('CustomerID')['InvoiceDay'].shift(1)\n",
    "tx_day_order['T2InvoiceDate'] = tx_day_order.groupby('CustomerID')['InvoiceDay'].shift(2)\n",
    "tx_day_order['T3InvoiceDate'] = tx_day_order.groupby('CustomerID')['InvoiceDay'].shift(3)\n",
    "\n",
    "tx_day_order.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will base on 4 nearest purchasing days to build the model \n",
    "tx_day_order['DayDiff'] = (tx_day_order['InvoiceDay'] - tx_day_order['PrevInvoiceDate']).dt.days\n",
    "tx_day_order['DayDiff2'] = (tx_day_order['InvoiceDay'] - tx_day_order['T2InvoiceDate']).dt.days\n",
    "tx_day_order['DayDiff3'] = (tx_day_order['InvoiceDay'] - tx_day_order['T3InvoiceDate']).dt.days\n",
    "\n",
    "tx_day_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1643f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_day_diff = tx_day_order.groupby('CustomerID').agg({'DayDiff': ['mean', 'std']}).reset_index()\n",
    "tx_day_diff.columns = ['CustomerID', 'DayDiffMean', 'DayDiffStd']\n",
    "tx_day_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_day_order_last = tx_day_order.drop_duplicates(subset = ['CustomerID'], keep ='last')\n",
    "tx_day_order_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_day_order_last = tx_day_order_last.dropna()\n",
    "tx_day_order_last = pd.merge(tx_day_order_last, tx_day_diff, on = 'CustomerID')\n",
    "tx_user = pd.merge(tx_user, tx_day_order_last[['CustomerID', 'DayDiff', 'DayDiff2', 'DayDiff3', 'DayDiffMean', 'DayDiffStd']], on ='CustomerID')\n",
    "\n",
    "tx_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c91c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_day_order_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ee33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the label\n",
    "import pandas as pd\n",
    "tx_class = tx_user.copy()\n",
    "tx_class = pd.get_dummies(tx_class)\n",
    "\n",
    "tx_class.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99775064",
   "metadata": {},
   "source": [
    "Sửa threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b17d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_class['NextPurchaseDayRange'] = 1\n",
    "tx_class.loc[tx_class.NextPurchaseDay >90, 'NextPurchaseDayRange'] = 0\n",
    "\n",
    "\n",
    "\n",
    "tx_class.NextPurchaseDayRange.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "corr = tx_class[tx_class.columns].corr()\n",
    "plt.figure(figsize = (30,20))\n",
    "sns.heatmap(corr, annot =True, linewidths = 0.2, fmt = '.2f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_class = tx_class.drop('NextPurchaseDay', axis = 1)\n",
    "X, y = tx_class.drop('NextPurchaseDayRange', axis = 1), tx_class.NextPurchaseDayRange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e784e",
   "metadata": {},
   "source": [
    "thay đổi random state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b87f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=52)\n",
    "models = []\n",
    "models.append((\"LR\", LogisticRegression()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('SVC',SVC()))\n",
    "models.append(('Dtree', DecisionTreeClassifier()))\n",
    "models.append(('XGB', xgb.XGBClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=2)\n",
    "    cv_result = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"accuracy\")\n",
    "    print(name, cv_result)\n",
    "                \n",
    "ltv_xgb_model = xgb.XGBClassifier(learning_rate= 0.01, max_depth= 3, n_estimators=50).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of XGB Classifier on training set: {:.2f}'.format(ltv_xgb_model.score(X_train, y_train)))\n",
    "print('Accuracy of XGB Classifier on test set : {:.2f}'.format(ltv_xgb_model.score(X_test[X_train.columns], y_test)))\n",
    "\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06688b",
   "metadata": {},
   "source": [
    "Kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = ltv_xgb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2fd9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in range(30, 61):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=state)\n",
    "    xgb_model = xgb.XGBClassifier().fit(X_train, y_train)\n",
    "    print('Random State:', state)\n",
    "    print('Accuracy of XGB Classifier on training set: {:.2f}'.format(xgb_model.score(X_train, y_train)))\n",
    "    print('Accuracy of XGB Classifier on test set : {:.2f}'.format(xgb_model.score(X_test[X_train.columns], y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c1936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121bfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the XGBClassifier\n",
    "xgb_model = XGBClassifier(learning_rate=0.1, max_depth= 3, n_estimators= 50)\n",
    "\n",
    "# Set up the random seed\n",
    "np.random.seed(52)\n",
    "\n",
    "# Define the number of iterations to run the model\n",
    "n_iterations = 100\n",
    "\n",
    "# Create empty lists to store accuracy scores for train and test sets\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Loop through the number of iterations\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # Randomly split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the accuracy score for the training data\n",
    "    train_score = xgb_model.score(X_train, y_train)\n",
    "    \n",
    "    # Calculate the accuracy score for the testing data\n",
    "    test_score = xgb_model.score(X_test, y_test)\n",
    "    \n",
    "    # Add the accuracy scores to the respective lists\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "# Calculate the mean accuracy score for the training data\n",
    "mean_train_score = np.mean(train_scores)\n",
    "\n",
    "# Calculate the mean accuracy score for the testing data\n",
    "mean_test_score = np.mean(test_scores)\n",
    "\n",
    "# Print out the mean accuracy scores\n",
    "print(f\"Mean accuracy score for training set over {n_iterations} iterations: {mean_train_score:.4f}\")\n",
    "print(f\"Mean accuracy score for testing set over {n_iterations} iterations: {mean_test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [50, 100, 200],\n",
    "              'max_depth': [3, 5, 7],\n",
    "              'learning_rate': [0.01, 0.1, 0.3]}\n",
    "              \n",
    "xgb_model = XGBClassifier()\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "print('Best accuracy score:', grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
